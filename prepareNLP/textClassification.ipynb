{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2fdea8-9deb-4ab4-9eac-c895cf34f6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spappad\\AppData\\Local\\miniconda3\\envs\\prepareNLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer,TFBertMainLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa8e2e23-8d46-49ae-be01-20d97843d92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/train.tsv', sep = '\\t')[:50000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5724caf-adc8-4521-ab06-3df4e886171b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   PhraseId    50000 non-null  int64 \n",
      " 1   SentenceId  50000 non-null  int64 \n",
      " 2   Phrase      50000 non-null  object\n",
      " 3   Sentiment   50000 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbb76003-cdc2-4556-a008-4ba04f68abe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        188\n",
       "1         77\n",
       "2          8\n",
       "3          1\n",
       "4          6\n",
       "        ... \n",
       "49995     10\n",
       "49996     57\n",
       "49997      7\n",
       "49998     49\n",
       "49999     47\n",
       "Name: Phrase, Length: 50000, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Phrase'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b344aaa5-3e09-4d11-a035-f13c1225b2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Phrase'].str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f97936b2-2954-488d-b893-5e373e8c110f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.036"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Phrase'].str.len().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "090d72b9-feea-423c-be36-09c109b47ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "2    26581\n",
       "3    10423\n",
       "1     8214\n",
       "4     2746\n",
       "0     2036\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a93f89-07dd-4135-96cf-955d8c018a29",
   "metadata": {},
   "source": [
    "The sentiment lables are:<hr> <p>0 - Negative</p>  <p>1 - Somewhat neagtive</p>  <p>2 - neutral</p> <p>3 - somewhat positive</p>  <p>4 - positive</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6a2fdb1-eecf-431e-98fb-2171781ae71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)solve/main/vocab.txt: 100%|██████████████████████████████████████████| 213k/213k [00:00<00:00, 8.54MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████████████████████████████████████| 29.0/29.0 [00:00<00:00, 1.86kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|████████████████████████████████████████████| 570/570 [00:00<00:00, 36.5kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3228af3-bc6a-47a7-81c0-18f66567f9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Phrase'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8ba0abd-c070-4973-a524-4b44fef1983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max Length\n",
    "maxLength = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ab10da1-d07c-465c-9de4-122bee4627f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = tokenizer.encode_plus(\n",
    "    df['Phrase'].iloc[0], \n",
    "    max_length=maxLength, \n",
    "    truncation=True, \n",
    "    padding='max_length', \n",
    "    add_special_tokens=True,\n",
    "    return_tensors='tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f778dec6-6852-4a3d-b9e4-39e8241c9b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "951cb867-8d22-4463-8f31-8494a1773400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fe4b067-9645-4460-9f41-f8b8b0054308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       "array([[  101,   138,  1326,  1104, 13936, 25265, 16913, 15107,  1103,\n",
       "         8050,  2553,  1115,  1184,  1110,  1363,  1111,  1103, 20398,\n",
       "         1110,  1145,  1363,  1111,  1103,   176,  9900,   117,  1199,\n",
       "         1104,  1134,  5411,  1821, 14225,  1133,  3839,  1104,  1134,\n",
       "         7919,  1106,  1277,  1104,   170,  1642,   119,   102,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]])>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e942dce4-d17d-4091-9d7b-f2965443f86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3d6d6e5-d024-4362-a469-1331b0669517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d16f98e7-7bd4-4779-9299-cc296d78d623",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input_ids = np.zeros((len(df), maxLength))\n",
    "X_attn_masks = np.zeros((len(df), maxLength))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d56fa5f7-5729-412f-82ca-466efdd9dbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 128)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e86de3b-d859-4f9d-a06b-1ea72c4d90b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 128)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_attn_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0b8c687-293e-4de9-a193-dce6cd9897e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(df, ids, masks, tokenizer):\n",
    "    for i, text in tqdm(enumerate(df['Phrase'])):\n",
    "        tokenized_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=maxLength, \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            add_special_tokens=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        ids[i, :] = tokenized_text.input_ids\n",
    "        masks[i, :] = tokenized_text.attention_mask\n",
    "    return ids, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbdb744e-3e30-4d60-a885-2147992b7ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [00:24, 2064.30it/s]\n"
     ]
    }
   ],
   "source": [
    "X_input_ids, X_attn_masks = generate_training_data(df, X_input_ids, X_attn_masks, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "681c5fa9-1095-40ab-8480-15e8c87c1171",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.zeros((len(df), 5 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cf057eb-743f-491c-866d-b1f9444fd5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39656c15-1705-4935-8075-416e81a6c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[np.arange(len(df)), df['Sentiment'].values] = 1 # one-hot encoded target tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18abb7e8-9312-4933-8cbc-11c8e99dd3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d631a18-f12b-4bce-9f3a-5d5ebfa662f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     1     2 ... 49997 49998 49999] [1 2 2 ... 1 4 4]\n"
     ]
    }
   ],
   "source": [
    "print(np.arange(len(df)),df['Sentiment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f257630-18bc-4fad-be0b-10b60ab9ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a data pipeline using tensorflow dataset utility, creates batches of data for easy loading...\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96b28e82-8b5e-405e-bb41-e87fce7f1093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=(TensorSpec(shape=(128,), dtype=tf.float64, name=None), TensorSpec(shape=(128,), dtype=tf.float64, name=None), TensorSpec(shape=(5,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1) # one sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "511e5eb1-a84c-4c22-ad54-91e9d1f9d9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentimentDatasetMapFunction(input_ids, attn_masks, labels):\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attn_masks\n",
    "    }, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f31086b0-bab6-42d4-9ea9-2fb6f516e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(SentimentDatasetMapFunction) # converting to required format for tensorflow dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98658eb5-6754-40e2-9203-e3ae5e0ecaff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(128,), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(128,), dtype=tf.float64, name=None)}, TensorSpec(shape=(5,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53bed61a-14c1-41b9-a100-c3ad629e7ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(10000).batch(16, drop_remainder=True) # batch size, drop any left out tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b40d0929-02a1-490d-99f7-cad3d8b670d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(16, 128), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(16, 128), dtype=tf.float64, name=None)}, TensorSpec(shape=(16, 5), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a377ab4e-040e-42bd-9d76-d6d14be8e717",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.8\n",
    "train_size = int((len(df)//16)*p) # for each 16 batch of data we will have len(df)//16 samples, take 80% of that for train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83049b07-1d62-4ef6-a27d-a25db4c017c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f57b1ec4-7d8c-4b4a-a290-9b07b342f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d551ffc3-0c93-47b5-ae72-c1efdd995e69",
   "metadata": {},
   "source": [
    "<h2>Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77d674a3-879b-4ca5-b478-00674f0c1149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad8450f3-c36a-4840-9004-41aa32e6557f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|████████████████████████████████████████████████| 436M/436M [00:05<00:00, 75.7MB/s]\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b29dc738-c2a2-4864-861b-76a52d46fbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " bert (TFBertMainLayer)         TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " intermediate_layer (Dense)     (None, 256)          196864      ['bert[0][1]']                   \n",
      "                                                                                                  \n",
      " output_layer (Dense)           (None, 5)            1285        ['intermediate_layer[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,508,421\n",
      "Trainable params: 108,508,421\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# defining 2 input layers for input_ids and attn_masks\n",
    "input_ids = tf.keras.layers.Input(shape=(maxLength,), name='input_ids', dtype='int32')\n",
    "attn_masks = tf.keras.layers.Input(shape=(maxLength,), name='attention_mask', dtype='int32')\n",
    "\n",
    "bert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\n",
    "intermediate_layer = tf.keras.layers.Dense(256, activation='relu', name='intermediate_layer')(bert_embds)\n",
    "output_layer = tf.keras.layers.Dense(5, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes\n",
    "\n",
    "sentiment_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\n",
    "sentiment_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c281b74-2f18-4e26-a66e-fafdda9886b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43430704-016a-4dfc-9f5b-86f7663f1369",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model.compile(optimizer=optim, loss=loss_func, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4735abab-4f9f-42c7-9adb-0f555b4e60d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 159/2500 [>.............................] - ETA: 20:46 - loss: 1.1611 - accuracy: 0.5326"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist = sentiment_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72ab8d17-ddff-40ae-b440-de48b73dd116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sentiment_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sentiment_model\\assets\n"
     ]
    }
   ],
   "source": [
    "sentiment_model.save('sentiment_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85bda96c-8b32-4203-a93e-ac98a759a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model in h5 format\n",
    "sentiment_model.save('sentiment_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897339a8-bb93-4b00-ba43-4dd95b5e9b97",
   "metadata": {},
   "source": [
    "<h2> Full Code with Tensor Pipeline</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e8129c-f3df-44aa-99de-7bbef3cad4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PrepareUser\\miniconda3\\envs\\prepareNLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100000it [00:42, 2329.50it/s]\n",
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  27/5000 [..............................] - ETA: 42:50 - loss: 1.3676 - accuracy: 0.4815"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertMainLayer, TFBertModel\n",
    "\n",
    "# Reading in a TSV file containing training data and selecting only the first 50,000 rows\n",
    "df = pd.read_csv('dataset/train.tsv', sep='\\t')[:100000]\n",
    "\n",
    "# Creating a tokenizer object from the BERT-base-cased model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# Setting the maximum sequence length for the tokenized input\n",
    "maxLength = 128\n",
    "\n",
    "# Defining a function to generate the training data\n",
    "def generate_training_data(df, ids, masks, tokenizer):\n",
    "    for i, text in tqdm(enumerate(df['Phrase'])):\n",
    "        # Tokenizing the input text using the BERT tokenizer\n",
    "        tokenized_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=maxLength,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        # Storing the tokenized input and attention masks in numpy arrays\n",
    "        ids[i, :] = tokenized_text.input_ids\n",
    "        masks[i, :] = tokenized_text.attention_mask\n",
    "    return ids, masks\n",
    "\n",
    "# Creating numpy arrays to store the tokenized input and attention masks\n",
    "X_input_ids = np.zeros((len(df), maxLength), dtype=np.int32)\n",
    "X_attn_masks = np.zeros((len(df), maxLength), dtype=np.int32)\n",
    "\n",
    "# Generating the training data using the generate_training_data function\n",
    "X_input_ids, X_attn_masks = generate_training_data(df, X_input_ids, X_attn_masks, tokenizer)\n",
    "\n",
    "# Convert the labels to one-hot encoded vectors\n",
    "labels = tf.keras.utils.to_categorical(df['Sentiment'], num_classes=5)\n",
    "\n",
    "# Creating a TensorFlow dataset object from the input and target data\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n",
    "\n",
    "# Defining a function to map the input and target data to the required format for the TensorFlow dataset\n",
    "def SentimentDatasetMapFunction(input_ids, attn_masks, labels):\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attn_masks\n",
    "    }, labels\n",
    "\n",
    "# Mapping the dataset to the required format using the SentimentDatasetMapFunction\n",
    "dataset = dataset.map(SentimentDatasetMapFunction)\n",
    "\n",
    "# Shuffling and batching the dataset for training the model\n",
    "dataset = dataset.shuffle(10000).batch(16, drop_remainder=True)\n",
    "\n",
    "# Splitting the dataset into training and validation sets\n",
    "p = 0.8\n",
    "train_size = int((len(df) // 16) * p)\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)\n",
    "\n",
    "# Creating a BERT model object and defining the input layers\n",
    "model = TFBertModel.from_pretrained('bert-base-cased')\n",
    "input_ids = tf.keras.layers.Input(shape=(maxLength,), name='input_ids', dtype='int32')\n",
    "attn_masks = tf.keras.layers.Input(shape=(maxLength,), name='attention_mask', dtype='int32')\n",
    "\n",
    "# Feeding the input layers to the BERT model and adding a dense layer and output layer\n",
    "bert_embds = model.bert(input_ids, attention_mask=attn_masks)[1]\n",
    "intermediate_layer = tf.keras.layers.Dense(256, activation='relu', name='intermediate_layer')(bert_embds)\n",
    "output_layer = tf.keras.layers.Dense(5, activation='softmax', name='output_layer')(intermediate_layer)\n",
    "\n",
    "# Creating a TensorFlow model object with the input and output layers\n",
    "sentiment_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\n",
    "\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "sentiment_model.compile(optimizer=optim, loss=loss_func, metrics=[acc])\n",
    "hist = sentiment_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd889895-9bdc-4b93-936f-ad00bb1f878b",
   "metadata": {},
   "source": [
    "<h2>Prediction</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1fbbf450-472e-4a42-a7b3-71ed0a927544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_model = tf.keras.models.load_model('sentiment_model')\n",
    "sentiment_model = tf.keras.models.load_model('sentiment_model.h5', custom_objects={\"TFBertMainLayer\":TFBertMainLayer})\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "def prepare_data(input_text, tokenizer):\n",
    "    token = tokenizer.encode_plus(\n",
    "        input_text,\n",
    "        max_length=maxLength, \n",
    "        truncation=True, \n",
    "        padding='max_length', \n",
    "        add_special_tokens=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    return {\n",
    "        'input_ids': tf.cast(token.input_ids, tf.float64),\n",
    "        'attention_mask': tf.cast(token.attention_mask, tf.float64)\n",
    "    }\n",
    "\n",
    "def make_prediction(model, processed_data, classes=['Negative', 'A bit negative', 'Neutral', 'A bit positive', 'Positive']):\n",
    "    probs = model.predict(processed_data)[0]\n",
    "    threshold = 0.50\n",
    "    if np.max(probs) < threshold:\n",
    "        prediction = 'Fallback Class'\n",
    "    else:\n",
    "        prediction = classes[np.argmax(probs)]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a506d8dc-201b-4c6d-b70e-caca192eb88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter movie review here:  its a good movie\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "Predicted Sentiment: A bit positive\n"
     ]
    }
   ],
   "source": [
    "maxLength = 128\n",
    "input_text = input('Enter movie review here: ')\n",
    "processed_data = prepare_data(input_text, tokenizer)\n",
    "result = make_prediction(sentiment_model, processed_data=processed_data)\n",
    "print(f\"Predicted Sentiment: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f0a2445-a94e-4935-9b36-3eca8e49774e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 69ms/step\n",
      "[3.3538698e-04 4.3500261e-04 4.0685800e-03 9.5507726e-02 8.9965338e-01]\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "#Predicting without function\n",
    "maxLength = 128\n",
    "input_text = \"very very good movie\"\n",
    "token = tokenizer.encode_plus(input_text, max_length=maxLength,truncation=True,padding='max_length', add_special_tokens=True,return_tensors='tf')\n",
    "processed_data= {'input_ids': tf.cast(token.input_ids, tf.float64),'attention_mask': tf.cast(token.attention_mask, tf.float64)}\n",
    "classes=['Negative', 'A bit negative', 'Neutral', 'A bit positive', 'Positive']\n",
    "probs = sentiment_model.predict(processed_data)[0]\n",
    "# Set a threshold below which all predictions will be considered as the fallback class\n",
    "threshold = 0.55\n",
    "if np.max(probs) < threshold:\n",
    "    prediction = 'Fallback Class'\n",
    "else:\n",
    "    prediction = classes[np.argmax(probs)]\n",
    "print(probs)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c86be9a-51b0-4252-8b31-dcc5d1918089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
