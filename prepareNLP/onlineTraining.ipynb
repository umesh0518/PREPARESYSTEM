{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae76cf0a-fec6-4a08-8533-e5db78958f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prepare User\\miniconda3\\envs\\prepareNLU\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import TFBertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4683bbbb-9fa3-4be7-a1fe-9414f0bb8de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFBertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "554a5b6c-4422-467f-9820-16dbe0230ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the maximum sequence length (should match the pre-trained model)\n",
    "max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b83ef80-3b7b-4435-9b52-ee54b334a41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " bert (TFBertMainLayer)         TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " output_layer (Dense)           (None, 3)            2307        ['bert[0][1]']                   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,312,579\n",
      "Trainable params: 108,312,579\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define a model\n",
    "# defining 2 input layers for input_ids and attn_masks\n",
    "input_ids = tf.keras.layers.Input(shape=(max_seq_length,), name='input_ids', dtype='int32')\n",
    "attn_masks = tf.keras.layers.Input(shape=(max_seq_length,), name='attention_mask', dtype='int32')\n",
    "bert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\n",
    "output_layer = tf.keras.layers.Dense(3, activation='softmax', name='output_layer')(bert_embds)\n",
    "# Freeze BERT layers\n",
    "# model.bert.trainable = False\n",
    "final_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\n",
    "final_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb08d650-baed-4968-b1e6-100b095d1783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7fa18c3-fdce-4179-bc92-693dcd720620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop\n",
    "epochs = 10\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0de5be03-5b94-4bb3-b8f7-a26f30d7eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using online learning\n",
    "dataset = {\n",
    "    \"abominable\": \"Negative\",\n",
    "    \"assure\": \"A bit positive\",\n",
    "    \"atrocious\": \"Negative\",\n",
    "    \"average\": \"Neutral\",\n",
    "    \"awful\": \"Negative\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "828d287f-840a-4d6d-b4fc-72d24613e001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prepare User\\miniconda3\\envs\\prepareNLU\\lib\\site-packages\\keras\\backend.py:5531: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch: 1 / 5\n",
      "  Loss: 1.5004086\n",
      "  Batch: 2 / 5\n",
      "  Loss: 2.3184364\n",
      "  Batch: 3 / 5\n",
      "  Loss: 1.1669056\n",
      "  Batch: 4 / 5\n",
      "  Loss: 0.7315926\n",
      "  Batch: 5 / 5\n",
      "  Loss: 0.9992112\n",
      "Epoch: 2\n",
      "  Batch: 1 / 5\n",
      "  Loss: 0.4491219\n",
      "  Batch: 2 / 5\n",
      "  Loss: 1.93696\n",
      "  Batch: 3 / 5\n",
      "  Loss: 0.44368446\n",
      "  Batch: 4 / 5\n",
      "  Loss: 0.49495578\n",
      "  Batch: 5 / 5\n",
      "  Loss: 0.50102514\n",
      "Epoch: 3\n",
      "  Batch: 1 / 5\n",
      "  Loss: 0.21192998\n",
      "  Batch: 2 / 5\n",
      "  Loss: 1.6279416\n",
      "  Batch: 3 / 5\n",
      "  Loss: 0.21872535\n",
      "  Batch: 4 / 5\n",
      "  Loss: 0.48704636\n",
      "  Batch: 5 / 5\n",
      "  Loss: 0.26618335\n",
      "Epoch: 4\n",
      "  Batch: 1 / 5\n",
      "  Loss: 0.12260947\n",
      "  Batch: 2 / 5\n",
      "  Loss: 1.1525233\n",
      "  Batch: 3 / 5\n",
      "  Loss: 0.13130438\n",
      "  Batch: 4 / 5\n",
      "  Loss: 0.422589\n",
      "  Batch: 5 / 5\n",
      "  Loss: 0.1656648\n",
      "Epoch: 5\n",
      "  Batch: 1 / 5\n",
      "  Loss: 0.08321707\n",
      "  Batch: 2 / 5\n",
      "  Loss: 0.65288585\n",
      "  Batch: 3 / 5\n",
      "  Loss: 0.0897254\n",
      "  Batch: 4 / 5\n",
      "  Loss: 0.36421108\n",
      "  Batch: 5 / 5\n",
      "  Loss: 0.107005976\n",
      "Epoch: 6\n",
      "  Batch: 1 / 5\n",
      "  Loss: 0.060469553\n",
      "  Batch: 2 / 5\n",
      "  Loss: 0.2790986\n",
      "  Batch: 3 / 5\n",
      "  Loss: 0.06254729\n",
      "  Batch: 4 / 5\n",
      "  Loss: 0.25365698\n",
      "  Batch: 5 / 5\n",
      "  Loss: 0.08322825\n",
      "Epoch: 7\n",
      "  Batch: 1 / 5\n",
      "  Loss: 0.04496528\n",
      "  Batch: 2 / 5\n",
      "  Loss: 0.092065595\n",
      "  Batch: 3 / 5\n",
      "  Loss: 0.044515233\n",
      "  Batch: 4 / 5\n",
      "  Loss: 0.20235594\n",
      "  Batch: 5 / 5\n",
      "  Loss: 0.07569766\n",
      "Epoch: 8\n",
      "  Batch: 1 / 5\n",
      "  Loss: 0.032974012\n",
      "  Batch: 2 / 5\n",
      "  Loss: 0.015532356\n",
      "  Batch: 3 / 5\n",
      "  Loss: 0.032603577\n",
      "  Batch: 4 / 5\n",
      "  Loss: 0.14657968\n",
      "  Batch: 5 / 5\n",
      "  Loss: 0.042694286\n",
      "Epoch: 9\n",
      "  Batch: 1 / 5\n",
      "  Loss: 0.025176637\n",
      "  Batch: 2 / 5\n",
      "  Loss: 0.010548904\n",
      "  Batch: 3 / 5\n",
      "  Loss: 0.024697823\n",
      "  Batch: 4 / 5\n",
      "  Loss: 0.10065381\n",
      "  Batch: 5 / 5\n",
      "  Loss: 0.037412908\n",
      "Epoch: 10\n",
      "  Batch: 1 / 5\n",
      "  Loss: 0.019894816\n",
      "  Batch: 2 / 5\n",
      "  Loss: 0.009597707\n",
      "  Batch: 3 / 5\n",
      "  Loss: 0.019369505\n",
      "  Batch: 4 / 5\n",
      "  Loss: 0.062970825\n",
      "  Batch: 5 / 5\n",
      "  Loss: 0.030379014\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print('Epoch:', epoch+1)\n",
    "    for i, (word, label) in enumerate(dataset.items()):\n",
    "        # Prepare the input data\n",
    "        inputs = tokenizer.encode_plus(word, add_special_tokens=True, max_length=max_seq_length, padding='max_length', truncation=True, return_tensors='tf')\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        y_true = np.zeros((1,3))\n",
    "        if label == \"Negative\":\n",
    "            y_true[0,0] = 1\n",
    "        elif label == \"A bit positive\":\n",
    "            y_true[0,1] = 1\n",
    "        else:\n",
    "            y_true[0,2] = 1\n",
    "        \n",
    "        # Perform a single online learning update\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = final_model([input_ids, attention_mask])\n",
    "            loss_value = loss_function(y_true, logits)\n",
    "        gradients = tape.gradient(loss_value, final_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, final_model.trainable_variables))\n",
    "        \n",
    "        # Print progress\n",
    "        if (i+1) % batch_size == 0:\n",
    "            print('  Batch:', i+1, '/', len(dataset))\n",
    "            print('  Loss:', loss_value.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491e1f19-3ec0-4b9f-b917-ceea4feb76e4",
   "metadata": {},
   "source": [
    "<h2>Online Training </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc2fb6d-8e84-4e7d-b765-7bd328492ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFBertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "#Define the maximum sequence length (should match the pre-trained model)\n",
    "max_seq_length = 128\n",
    "\n",
    "#Define a model\n",
    "# defining 2 input layers for input_ids and attn_masks\n",
    "input_ids = tf.keras.layers.Input(shape=(max_seq_length,), name='input_ids', dtype='int32')\n",
    "attn_masks = tf.keras.layers.Input(shape=(max_seq_length,), name='attention_mask', dtype='int32')\n",
    "bert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\n",
    "output_layer = tf.keras.layers.Dense(3, activation='softmax', name='output_layer')(bert_embds)\n",
    "# Freeze BERT layers\n",
    "# model.bert.trainable = False\n",
    "final_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "\n",
    "# Define training loop\n",
    "epochs = 10\n",
    "batch_size = 1\n",
    "\n",
    "# Train the model using online learning\n",
    "dataset = {\n",
    "    \"abominable\": \"Negative\",\n",
    "    \"assure\": \"A bit positive\",\n",
    "    \"atrocious\": \"Negative\",\n",
    "    \"average\": \"Neutral\",\n",
    "    \"awful\": \"Negative\"\n",
    "}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch:', epoch+1)\n",
    "    for i, (word, label) in enumerate(dataset.items()):\n",
    "        # Prepare the input data\n",
    "        inputs = tokenizer.encode_plus(word, add_special_tokens=True, max_length=max_seq_length, padding='max_length', truncation=True, return_tensors='tf')\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        y_true = np.zeros((1,3))\n",
    "        if label == \"Negative\":\n",
    "            y_true[0,0] = 1\n",
    "        elif label == \"A bit positive\":\n",
    "            y_true[0,1] = 1\n",
    "        else:\n",
    "            y_true[0,2] = 1\n",
    "        \n",
    "        # Perform a single online learning update\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = final_model([input_ids, attention_mask])\n",
    "            loss_value = loss_function(y_true, logits)\n",
    "        gradients = tape.gradient(loss_value, final_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, final_model.trainable_variables))\n",
    "        \n",
    "        # Print progress\n",
    "        if (i+1) % batch_size == 0:\n",
    "            print('  Batch:', i+1, '/', len(dataset))\n",
    "            print('  Loss:', loss_value.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d325d-c5ab-4b69-a453-1da0165996fa",
   "metadata": {},
   "source": [
    "<h2>Full Training on PHEE_dataset using Bio_ClinicalBERT</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f9845a-2bd8-427f-89f0-cacbee8b22b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PrepareUser\\miniconda3\\envs\\prepareNLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52b3eec9-16df-4bb0-84fe-aed9e17a63d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file as a Python object\n",
    "with open('PHEE_dataset/train.json') as f:\n",
    "    data = json.loads(f.read())\n",
    "# Initialize empty lists to store the extracted features\n",
    "contexts = []\n",
    "event_types = []\n",
    "# Extract the features for each entry in the object\n",
    "for entry in data:\n",
    "    contexts.append(entry[\"context\"])\n",
    "    events = entry[\"annotations\"][0][\"events\"]\n",
    "    event_types.append(events[0][\"event_type\"])\n",
    "# Create a pandas dataframe using the extracted features\n",
    "df = pd.DataFrame({\"context\": contexts, \"event_type\": event_types})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d8ec1f-0cdc-47d3-8cd4-27eed83ccece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event_type\n",
      "Adverse_event                  3509\n",
      "Potential_therapeutic_event     350\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "event_type_counts = df['event_type'].value_counts()\n",
    "print(event_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "379d1fa6-4509-48f2-93c8-606657f5b8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adverse_event' 'Potential_therapeutic_event']\n"
     ]
    }
   ],
   "source": [
    "# Get the unique event_type values from your dataframe\n",
    "event_types = df['event_type'].unique()\n",
    "print(event_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c2b5343-ce53-419e-9101-6aa883a49983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the event_type column to a categorical column\n",
    "df['event_type_categorical'] = pd.Categorical(df['event_type'], categories=event_types)\n",
    "# Convert the categorical column to one-hot encoding using tf.keras.utils.to_categorical()\n",
    "labels = tf.keras.utils.to_categorical(df['event_type_categorical'].cat.codes, num_classes=len(event_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bd2cf44-340b-4a44-9706-3f444abd5532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event_type_categorical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OBJECTIVE: To test the hypothesis that tumor n...</td>\n",
       "      <td>Adverse_event</td>\n",
       "      <td>Adverse_event</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>An evaluation of ovarian structure and functio...</td>\n",
       "      <td>Adverse_event</td>\n",
       "      <td>Adverse_event</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Phenobarbital hepatotoxicity in an 8-month-old...</td>\n",
       "      <td>Adverse_event</td>\n",
       "      <td>Adverse_event</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The authors report a case of Balint syndrome w...</td>\n",
       "      <td>Adverse_event</td>\n",
       "      <td>Adverse_event</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>According to the Naranjo probability scale, fl...</td>\n",
       "      <td>Adverse_event</td>\n",
       "      <td>Adverse_event</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context     event_type   \n",
       "0  OBJECTIVE: To test the hypothesis that tumor n...  Adverse_event  \\\n",
       "1  An evaluation of ovarian structure and functio...  Adverse_event   \n",
       "2  Phenobarbital hepatotoxicity in an 8-month-old...  Adverse_event   \n",
       "3  The authors report a case of Balint syndrome w...  Adverse_event   \n",
       "4  According to the Naranjo probability scale, fl...  Adverse_event   \n",
       "\n",
       "  event_type_categorical  \n",
       "0          Adverse_event  \n",
       "1          Adverse_event  \n",
       "2          Adverse_event  \n",
       "3          Adverse_event  \n",
       "4          Adverse_event  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24d4d728-3fca-4d3d-9dff-862e7b7f9ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a tokenizer object from the BERT-base-cased model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "# Setting the maximum sequence length for the tokenized input\n",
    "maxLength = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca4e0daa-1f81-4008-b6cc-7292296d5388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to generate the training data\n",
    "def generate_training_data(df, ids, masks, tokenizer):\n",
    "    for i, text in tqdm(enumerate(df['context'])):\n",
    "        # Tokenizing the input text using the BERT tokenizer\n",
    "        tokenized_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=maxLength,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        # Storing the tokenized input and attention masks in numpy arrays\n",
    "        ids[i, :] = tokenized_text.input_ids\n",
    "        masks[i, :] = tokenized_text.attention_mask\n",
    "    return ids, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4324fc6-724e-42f7-8f04-f23e869a9e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating numpy arrays to store the tokenized input and attention masks\n",
    "X_input_ids = np.zeros((len(df), maxLength), dtype=np.int32)\n",
    "X_attn_masks = np.zeros((len(df), maxLength), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19dba9b8-7060-49b9-afc4-9bf41a8d0f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3859it [00:02, 1804.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generating the training data using the generate_training_data function\n",
    "X_input_ids, X_attn_masks = generate_training_data(df, X_input_ids, X_attn_masks, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fee5f79f-a8ed-4dfe-a4f7-f0eb09ec3e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a TensorFlow dataset object from the input and target data\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8f337be-19ad-4c6d-83f4-3dc2864cc627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to map the input and target data to the required format for the TensorFlow dataset\n",
    "def MapFunction(input_ids, attn_masks, labels):\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attn_masks\n",
    "    }, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be008fd9-5cc3-4539-a550-30b057f3c0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the dataset to the required format using the SentimentDatasetMapFunction\n",
    "dataset = dataset.map(MapFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c17d640-ac72-4186-a6e7-624c62b7764f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': TensorSpec(shape=(128,), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(128,), dtype=tf.int32, name=None)}, TensorSpec(shape=(2,), dtype=tf.float32, name=None))\n"
     ]
    }
   ],
   "source": [
    "print(dataset.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b4e8ffe-4cf2-4d30-870e-d9a1735f035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling and batching the dataset for training the model\n",
    "dataset = dataset.shuffle(1000).batch(32, drop_remainder=True)\n",
    "# Splitting the dataset into training and validation sets\n",
    "p = 0.8\n",
    "train_size = int((len(df) // 32) * p)\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62da8278-0305-4609-83d9-f2c884e3ef81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Creating a BERT model object and defining the input layers\n",
    "BertModel = TFAutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "input_ids = tf.keras.layers.Input(shape=(maxLength,), name='input_ids', dtype='int32')\n",
    "attn_masks = tf.keras.layers.Input(shape=(maxLength,), name='attention_mask', dtype='int32')\n",
    "\n",
    "# Feeding the input layers to the BERT model and adding a dense layer and output layer\n",
    "bert_embds = BertModel.bert(input_ids, attention_mask=attn_masks)[1]\n",
    "output_layer = tf.keras.layers.Dense(2, activation='softmax', name='output_layer')(bert_embds)\n",
    "# Creating a TensorFlow model object with the input and output layers\n",
    "model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30e73723-3aa5-474a-85df-98f98f8fad1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " bert (TFBertMainLayer)         TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " output_layer (Dense)           (None, 2)            1538        ['bert[0][1]']                   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,311,810\n",
      "Trainable params: 108,311,810\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d5cc83-9224-4346-8ba3-c11a24e1d55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 6/96 [>.............................] - ETA: 1:21 - loss: 0.3331 - accuracy: 0.9167"
     ]
    }
   ],
   "source": [
    "optim = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optim, loss=loss_func, metrics=[acc])\n",
    "hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9615cf66-46e4-40f2-b42c-94801304dc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('phee_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587685a0-670c-42ce-820c-454872dde0ee",
   "metadata": {},
   "source": [
    "<h2>Full Training on PHEE_dataset using Bio_ClinicalBERT(FULL Code)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40242d60-7329-4456-92a3-e31ca396a376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PrepareUser\\miniconda3\\envs\\prepareNLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adverse_event' 'Potential_therapeutic_event']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3859it [00:01, 2553.09it/s]\n",
      "Some layers from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 4/96 [>.............................] - ETA: 1:24 - loss: 0.8247 - accuracy: 0.4219"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "# Load the JSON file as a Python object\n",
    "with open('PHEE_dataset/train.json') as f:\n",
    "    data = json.loads(f.read())\n",
    "# Initialize empty lists to store the extracted features\n",
    "contexts = []\n",
    "event_types = []\n",
    "# Extract the features for each entry in the object\n",
    "for entry in data:\n",
    "    contexts.append(entry[\"context\"])\n",
    "    events = entry[\"annotations\"][0][\"events\"]\n",
    "    event_types.append(events[0][\"event_type\"])\n",
    "# Create a pandas dataframe using the extracted features\n",
    "df = pd.DataFrame({\"context\": contexts, \"event_type\": event_types})\n",
    "\n",
    "# Get the unique event_type values from your dataframe\n",
    "event_types = df['event_type'].unique()\n",
    "# event_types_sorted = sorted(event_types)\n",
    "\n",
    "print(event_types)\n",
    "#Savng event_types attribute inside model\n",
    "custom_att = tf.Variable(['custom_att1', 'custom_att2'], trainable=False)\n",
    "\n",
    "# Convert the event_type column to a categorical column\n",
    "df['event_type_categorical'] = pd.Categorical(df['event_type'], categories=event_types)\n",
    "# Convert the categorical column to one-hot encoding using tf.keras.utils.to_categorical()\n",
    "labels = tf.keras.utils.to_categorical(df['event_type_categorical'].cat.codes, num_classes=len(event_types))\n",
    "\n",
    "# Creating a tokenizer object from the BERT-base-cased model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "# Setting the maximum sequence length for the tokenized input\n",
    "maxLength = 128\n",
    "# Defining a function to generate the training data\n",
    "def generate_training_data(df, ids, masks, tokenizer):\n",
    "    for i, text in tqdm(enumerate(df['context'])):\n",
    "        # Tokenizing the input text using the BERT tokenizer\n",
    "        tokenized_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=maxLength,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        # Storing the tokenized input and attention masks in numpy arrays\n",
    "        ids[i, :] = tokenized_text.input_ids\n",
    "        masks[i, :] = tokenized_text.attention_mask\n",
    "    return ids, masks\n",
    "\n",
    "# Creating numpy arrays to store the tokenized input and attention masks\n",
    "X_input_ids = np.zeros((len(df), maxLength), dtype=np.int32)\n",
    "X_attn_masks = np.zeros((len(df), maxLength), dtype=np.int32)\n",
    "\n",
    "# Generating the training data using the generate_training_data function\n",
    "X_input_ids, X_attn_masks = generate_training_data(df, X_input_ids, X_attn_masks, tokenizer)\n",
    "\n",
    "# Creating a TensorFlow dataset object from the input and target data\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n",
    "\n",
    "# Defining a function to map the input and target data to the required format for the TensorFlow dataset\n",
    "def MapFunction(input_ids, attn_masks, labels):\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attn_masks\n",
    "    }, labels\n",
    "\n",
    "# Mapping the dataset to the required format using the SentimentDatasetMapFunction\n",
    "dataset = dataset.map(MapFunction)\n",
    "# Shuffling and batching the dataset for training the model\n",
    "dataset = dataset.shuffle(1000).batch(32, drop_remainder=True)\n",
    "# Splitting the dataset into training and validation sets\n",
    "p = 0.8\n",
    "train_size = int((len(df) // 32) * p)\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)\n",
    "\n",
    "# Creating a BERT model object and defining the input layers\n",
    "BertModel = TFAutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "input_ids = tf.keras.layers.Input(shape=(maxLength,), name='input_ids', dtype='int32')\n",
    "attn_masks = tf.keras.layers.Input(shape=(maxLength,), name='attention_mask', dtype='int32')\n",
    "\n",
    "# Feeding the input layers to the BERT model and adding a dense layer and output layer\n",
    "bert_embds = BertModel.bert(input_ids, attention_mask=attn_masks)[1]\n",
    "output_layer = tf.keras.layers.Dense(2, activation='softmax', name='output_layer')(bert_embds)\n",
    "# Creating a TensorFlow model object with the input and output layers\n",
    "model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optim, loss=loss_func, metrics=[acc])\n",
    "\n",
    "\n",
    "hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs = 10\n",
    ")\n",
    "\n",
    "# model.save('phee_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec4d28d3-9ce0-4eda-82db-cdabec477586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PrepareUser\\miniconda3\\envs\\prepareNLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the saved file\n",
    "from tensorflow.keras.models import load_model\n",
    "lmodel = load_model('phee_model.h5',custom_objects={\"TFBertMainLayer\":TFBertMainLayer})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf55128-3701-4cef-b53a-6edb2b25c2d9",
   "metadata": {},
   "source": [
    "<h3> Evaluating model accurcay  on full test set </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bd26f4b-73ab-4e48-8460-1d1ac3aba986",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 28\u001b[0m\n\u001b[0;32m     24\u001b[0m X_attn_masks_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(df_test), maxLength), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Generating the test data using the generate_training_data function\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m X_input_ids_test, X_attn_masks_test \u001b[38;5;241m=\u001b[39m generate_training_data(df_test, X_input_ids_test, X_attn_masks_test, \u001b[43mtokenizer\u001b[49m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Creating a TensorFlow dataset object from the test input and target data\u001b[39;00m\n\u001b[0;32m     31\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((X_input_ids_test, X_attn_masks_test, labels_test))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the JSON file as a Python object\n",
    "with open('PHEE_dataset/test.json') as f:\n",
    "    data = json.loads(f.read())\n",
    "# Initialize empty lists to store the extracted features\n",
    "contexts = []\n",
    "event_types = []\n",
    "maxLength = 128\n",
    "# Extract the features for each entry in the object\n",
    "for entry in data:\n",
    "    contexts.append(entry[\"context\"])\n",
    "    events = entry[\"annotations\"][0][\"events\"]\n",
    "    event_types.append(events[0][\"event_type\"])\n",
    "# Create a pandas dataframe using the extracted features\n",
    "df_test = pd.DataFrame({\"context\": contexts, \"event_type\": event_types})\n",
    "# Get the unique event_type values from your dataframe\n",
    "event_types = df_test['event_type'].unique()\n",
    "# Convert the event_type column to a categorical column\n",
    "df_test['event_type_categorical'] = pd.Categorical(df_test['event_type'], categories=event_types)\n",
    "# Convert the categorical column to one-hot encoding using tf.keras.utils.to_categorical()\n",
    "labels_test = tf.keras.utils.to_categorical(df_test['event_type_categorical'].cat.codes, num_classes=len(event_types))\n",
    "\n",
    "# Creating numpy arrays to store the tokenized input and attention masks\n",
    "X_input_ids_test = np.zeros((len(df_test), maxLength), dtype=np.int32)\n",
    "X_attn_masks_test = np.zeros((len(df_test), maxLength), dtype=np.int32)\n",
    "\n",
    "\n",
    "# Generating the test data using the generate_training_data function\n",
    "X_input_ids_test, X_attn_masks_test = generate_training_data(df_test, X_input_ids_test, X_attn_masks_test, tokenizer)\n",
    "\n",
    "# Creating a TensorFlow dataset object from the test input and target data\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_input_ids_test, X_attn_masks_test, labels_test))\n",
    "\n",
    "# Mapping the test dataset to the required format using the MapFunction\n",
    "test_dataset = test_dataset.map(MapFunction)\n",
    "# Batching the dataset for evaluation\n",
    "test_dataset = test_dataset.batch(32)\n",
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = model.evaluate(test_dataset, verbose=1)\n",
    "print(f'Test loss: {loss}, Test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f79afe-5b25-46f2-979a-d3004d5d61de",
   "metadata": {},
   "source": [
    "<h2>Full Training on Dictionary training data (low dataset) using Bio_ClinicalBERT(FULL Code)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ae3d387-ffdf-4260-94d0-fa1e9eeb3655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PrepareUser\\miniconda3\\envs\\prepareNLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Training data dictionary\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "training_data = {\n",
    "        \"OBJECTIVE: To test the hypothesis that tumor necrosis factor (TNF)-alpha may mediate the loss and the dedifferentiation of subcutaneous fat tissue in the insulin-induced lipoatrophies of a diabetic patient who presented extensive lesions\": \"Adverse_event\",\n",
    "        \"An evaluation of ovarian structure and function should be considered in women of reproductive age being treated with valproate for epilepsy, especially if they develop menstrual cycle disturbances during treatment\": \"Adverse_event\",\n",
    "        \"Phenobarbital hepatotoxicity in an 8-month-old infant\": \"Adverse_event\",\n",
    "        \"The authors report a case of Balint syndrome with irreversible posterior leukoencephalopathy on MRI following intrathecal methotrexate and cytarabine\": \"Adverse_event\",\n",
    "        \"According to the Naranjo probability scale, flecainide was the probable cause of the patient's delirium; the Horn Drug Interaction Probability Scale indicates a possible pharmacokinetic drug interaction between flecainide and paroxetine\": \"Adverse_event\",\n",
    "        \"Contact dermatitis due to budesonide: report of five cases and review of the Japanese literature\": \"Adverse_event\",\n",
    "        \"Prolongation of the QT interval observed in a Japanese patient with vivax malaria following treatment with halofantrine\": \"Adverse_event\",\n",
    "        \"We report three cases of severe hepatotoxicity related to benzarone, a benzofuran derivative\": \"Adverse_event\",\n",
    "        \"Four patients who manifested symptoms of the antiepileptic drug (AED) hypersensitivity syndrome during therapy with carbamazepine are reported\": \"Adverse_event\",\n",
    "        \"Minocycline as a cause of drug-induced autoimmune hepatitis\": \"Adverse_event\",\n",
    "        \"Secondary acute myeloid leukemia after etoposide therapy for haemophagocytic lymphohistiocytosis\": \"Potential_therapeutic_event\",\n",
    "        \"Allergic contact angioedema to benzoyl peroxide\": \"Potential_therapeutic_event\",\n",
    "        \"A 60-year-old white man with chronic bronchitis was noted to develop acute respiratory failure and metabolic acidosis four days after being started on methazolamide (Neptazane) for an ophthalmologic problem\": \"Adverse_event\",\n",
    "        \"L-DOPA-induced excessive daytime sleepiness in PD: a placebo-controlled case with MSLT assessment\": \"Adverse_event\",\n",
    "        \"CONCLUSIONS: It is probable that foscarnet contributed to the electrolyte disorders and symptomatology in this patient\": \"Adverse_event\",\n",
    "        \"Acute drug induced hepatitis due to erlotinib\": \"Adverse_event\",\n",
    "        \"We observed 3 diabetic patients with intolerable dizziness followed by nausea and vomiting immediately after an initial administration of the alpha-glucosidase inhibitor, voglibose\": \"Adverse_event\",\n",
    "        \"Although risk factors for MTX-induced pulmonary toxicity are poorly understood, the presence in 3 out of 5 of our patients of pre-existing lung disease, represented by diffuse interstitial changes on chest X-ray, and mild bronchial asthma in two RA patients and by pulmonary silicosis in the patient with PsA may account for a predisposition to the development of MTX pneumonitis\": \"Adverse_event\",\n",
    "        \"Acute esmolol toxicity may be self-limiting because of its extremely short half-life\": \"Adverse_event\",\n",
    "        \"The association with prolonged unopposed estrogen-like stimulation with tamoxifen as a possible factor in the development of ovarian endometrioid carcinoma is discussed\": \"Adverse_event\",\n",
    "        \"The other woman had rheumatoid arthritis and developed acute tubular necrosis after treatment with gentamicin and cefoxitin\": \"Adverse_event\",\n",
    "        \"One patient who received clindamycin had liver biopsy findings of marked cholestasis, portal inflammation, bile duct injury and bile duct paucity (ductopenia)\": \"Adverse_event\",\n",
    "        \"A MEDLINE search (1966-January 2009) revealed one in vivo pharmacokinetic study on the interaction between flecainide, a CYP2D6 substrate, and paroxetine, a CYP2D6 inhibitor, as well as 3 case reports of flecainide-induced delirium\": \"Adverse_event\",\n",
    "        \"As these cases revealed, close monitoring of blood chemistry is mandatory after starting spironolactone, and patients should be advised to stop spironolactone immediately if diarrhoea develops\": \"Adverse_event\",\n",
    "        \"When these cells are exposed to nicotinic acid, an exaggerated immune response is produced that may lead to pain, redness, and swelling at the injection site\": \"Adverse_event\",\n",
    "        \"A case of heatstroke is reported in a 32-year-old man diagnosed with schizophrenia and on clozapine monotherapy\": \"Adverse_event\",\n",
    "        \"Paraplegia following intrathecal cytosine arabinoside\": \"Potential_therapeutic_event\",\n",
    "        \"Cimetidine-induced fever\": \"Potential_therapeutic_event\",\n",
    "        \"This case report illustrates the neurotoxicity unique to HDARAC\": \"Adverse_event\",\n",
    "        \"Agranulocytosis associated with ticlopidine: a possible benefit with filgastim\": \"Adverse_event\",\n",
    "        \"Since SS is a clinical diagnosis, heightened clinician awareness of the possibility of SS among patients receiving SSRI or mirtazapine in combination with opioids may lead to earlier detection and avoidance of potentially lethal consequences\": \"Adverse_event\",\n",
    "        \"The mechanism by which sunitinib induces gynaecomastia is thought to be associated with an unknown direct action on breast hormonal receptors\": \"Adverse_event\",\n",
    "        \"Intravitreal triamcinolone may have had an influence on the exacerbation of retinochoroiditis in the posterior pole of the patient\": \"Adverse_event\",\n",
    "        \"We evaluated a patient who developed a psychotic disorder after 4 months of isoniazid prophylaxis for a positive tuberculosis tine test\": \"Adverse_event\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabdae33-acfd-4e44-9a97-b12f781cfe3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adverse_event' 'Potential_therapeutic_event']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [00:01, 28.11it/s]\n",
      "Some layers from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 17s 614ms/step - loss: 0.7858 - accuracy: 0.3571 - val_loss: 0.3764 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 2s 221ms/step - loss: 0.4164 - accuracy: 0.8571 - val_loss: 0.3379 - val_accuracy: 0.8333\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2685 - accuracy: 0.9286"
     ]
    }
   ],
   "source": [
    "# Create a pandas dataframe using the dictionary\n",
    "df = pd.DataFrame(training_data.items(), columns=['context', 'event_type'])\n",
    "\n",
    "# Get the unique event_type values from your dataframe\n",
    "event_types = df['event_type'].unique()\n",
    "# event_types_sorted = sorted(event_types)\n",
    "\n",
    "print(event_types)\n",
    "\n",
    "# Convert the event_type column to a categorical column\n",
    "df['event_type_categorical'] = pd.Categorical(df['event_type'], categories=event_types)\n",
    "# Convert the categorical column to one-hot encoding using tf.keras.utils.to_categorical()\n",
    "labels = tf.keras.utils.to_categorical(df['event_type_categorical'].cat.codes, num_classes=len(event_types))\n",
    "\n",
    "# Creating a tokenizer object from the BERT-base-cased model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "# Setting the maximum sequence length for the tokenized input\n",
    "maxLength = 128\n",
    "# Defining a function to generate the training data\n",
    "def generate_training_data(df, ids, masks, tokenizer):\n",
    "    for i, text in tqdm(enumerate(df['context'])):\n",
    "        # Tokenizing the input text using the BERT tokenizer\n",
    "        tokenized_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=maxLength,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        # Storing the tokenized input and attention masks in numpy arrays\n",
    "        ids[i, :] = tokenized_text.input_ids\n",
    "        masks[i, :] = tokenized_text.attention_mask\n",
    "    return ids, masks\n",
    "\n",
    "# Creating numpy arrays to store the tokenized input and attention masks\n",
    "X_input_ids = np.zeros((len(df), maxLength), dtype=np.int32)\n",
    "X_attn_masks = np.zeros((len(df), maxLength), dtype=np.int32)\n",
    "\n",
    "# Generating the training data using the generate_training_data function\n",
    "X_input_ids, X_attn_masks = generate_training_data(df, X_input_ids, X_attn_masks, tokenizer)\n",
    "\n",
    "# Creating a TensorFlow dataset object from the input and target data\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n",
    "\n",
    "# Defining a function to map the input and target data to the required format for the TensorFlow dataset\n",
    "def MapFunction(input_ids, attn_masks, labels):\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attn_masks\n",
    "    }, labels\n",
    "\n",
    "# Mapping the dataset to the required format using the SentimentDatasetMapFunction\n",
    "dataset = dataset.map(MapFunction)\n",
    "# Shuffling and batching the dataset for training the model\n",
    "dataset = dataset.shuffle(len(df)).batch(4, drop_remainder=False)\n",
    "# Splitting the dataset into training and validation sets\n",
    "p = 0.9\n",
    "train_size = int((len(df) // 4) * p)\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)\n",
    "\n",
    "# Creating a BERT model object and defining the input layers\n",
    "BertModel = TFAutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "input_ids = tf.keras.layers.Input(shape=(maxLength,), name='input_ids', dtype='int32')\n",
    "attn_masks = tf.keras.layers.Input(shape=(maxLength,), name='attention_mask', dtype='int32')\n",
    "\n",
    "# Feeding the input layers to the BERT model and adding a dense layer and output layer\n",
    "bert_embds = BertModel.bert(input_ids, attention_mask=attn_masks)[1]\n",
    "output_layer = tf.keras.layers.Dense(len(event_types), activation='softmax', name='output_layer')(bert_embds)\n",
    "# Creating a TensorFlow model object with the input and output layers\n",
    "model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optim, loss=loss_func, metrics=[acc])\n",
    "hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs = 10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acfa12d-ecb2-4f9b-bb53-80314b189d8e",
   "metadata": {},
   "source": [
    "<h2>Full Training on Dictionary training data (low dataset) using Bio_ClinicalBERT and sub class method(Full Code)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c2ca042-ba2a-4ce7-b3d4-4a78696e3607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PrepareUser\\miniconda3\\envs\\prepareNLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Training data dictionary\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "training_data = {\n",
    "        \"OBJECTIVE: To test the hypothesis that tumor necrosis factor (TNF)-alpha may mediate the loss and the dedifferentiation of subcutaneous fat tissue in the insulin-induced lipoatrophies of a diabetic patient who presented extensive lesions\": \"Adverse_event\",\n",
    "        \"An evaluation of ovarian structure and function should be considered in women of reproductive age being treated with valproate for epilepsy, especially if they develop menstrual cycle disturbances during treatment\": \"Adverse_event\",\n",
    "        \"Phenobarbital hepatotoxicity in an 8-month-old infant\": \"Adverse_event\",\n",
    "        \"The authors report a case of Balint syndrome with irreversible posterior leukoencephalopathy on MRI following intrathecal methotrexate and cytarabine\": \"Adverse_event\",\n",
    "        \"According to the Naranjo probability scale, flecainide was the probable cause of the patient's delirium; the Horn Drug Interaction Probability Scale indicates a possible pharmacokinetic drug interaction between flecainide and paroxetine\": \"Adverse_event\",\n",
    "        \"Contact dermatitis due to budesonide: report of five cases and review of the Japanese literature\": \"Adverse_event\",\n",
    "        \"Prolongation of the QT interval observed in a Japanese patient with vivax malaria following treatment with halofantrine\": \"Adverse_event\",\n",
    "        \"We report three cases of severe hepatotoxicity related to benzarone, a benzofuran derivative\": \"Adverse_event\",\n",
    "        \"Four patients who manifested symptoms of the antiepileptic drug (AED) hypersensitivity syndrome during therapy with carbamazepine are reported\": \"Adverse_event\",\n",
    "        \"Minocycline as a cause of drug-induced autoimmune hepatitis\": \"Adverse_event\",\n",
    "        \"Secondary acute myeloid leukemia after etoposide therapy for haemophagocytic lymphohistiocytosis\": \"Potential_therapeutic_event\",\n",
    "        \"Allergic contact angioedema to benzoyl peroxide\": \"Potential_therapeutic_event\",\n",
    "        \"A 60-year-old white man with chronic bronchitis was noted to develop acute respiratory failure and metabolic acidosis four days after being started on methazolamide (Neptazane) for an ophthalmologic problem\": \"Adverse_event\",\n",
    "        \"L-DOPA-induced excessive daytime sleepiness in PD: a placebo-controlled case with MSLT assessment\": \"Adverse_event\",\n",
    "        \"CONCLUSIONS: It is probable that foscarnet contributed to the electrolyte disorders and symptomatology in this patient\": \"Adverse_event\",\n",
    "        \"Acute drug induced hepatitis due to erlotinib\": \"Adverse_event\",\n",
    "        \"We observed 3 diabetic patients with intolerable dizziness followed by nausea and vomiting immediately after an initial administration of the alpha-glucosidase inhibitor, voglibose\": \"Adverse_event\",\n",
    "        \"Although risk factors for MTX-induced pulmonary toxicity are poorly understood, the presence in 3 out of 5 of our patients of pre-existing lung disease, represented by diffuse interstitial changes on chest X-ray, and mild bronchial asthma in two RA patients and by pulmonary silicosis in the patient with PsA may account for a predisposition to the development of MTX pneumonitis\": \"Adverse_event\",\n",
    "        \"Acute esmolol toxicity may be self-limiting because of its extremely short half-life\": \"Adverse_event\",\n",
    "        \"The association with prolonged unopposed estrogen-like stimulation with tamoxifen as a possible factor in the development of ovarian endometrioid carcinoma is discussed\": \"Adverse_event\",\n",
    "        \"The other woman had rheumatoid arthritis and developed acute tubular necrosis after treatment with gentamicin and cefoxitin\": \"Adverse_event\",\n",
    "        \"One patient who received clindamycin had liver biopsy findings of marked cholestasis, portal inflammation, bile duct injury and bile duct paucity (ductopenia)\": \"Adverse_event\",\n",
    "        \"A MEDLINE search (1966-January 2009) revealed one in vivo pharmacokinetic study on the interaction between flecainide, a CYP2D6 substrate, and paroxetine, a CYP2D6 inhibitor, as well as 3 case reports of flecainide-induced delirium\": \"Adverse_event\",\n",
    "        \"As these cases revealed, close monitoring of blood chemistry is mandatory after starting spironolactone, and patients should be advised to stop spironolactone immediately if diarrhoea develops\": \"Adverse_event\",\n",
    "        \"When these cells are exposed to nicotinic acid, an exaggerated immune response is produced that may lead to pain, redness, and swelling at the injection site\": \"Adverse_event\",\n",
    "        \"A case of heatstroke is reported in a 32-year-old man diagnosed with schizophrenia and on clozapine monotherapy\": \"Adverse_event\",\n",
    "        \"Paraplegia following intrathecal cytosine arabinoside\": \"Potential_therapeutic_event\",\n",
    "        \"Cimetidine-induced fever\": \"Potential_therapeutic_event\",\n",
    "        \"This case report illustrates the neurotoxicity unique to HDARAC\": \"Adverse_event\",\n",
    "        \"Agranulocytosis associated with ticlopidine: a possible benefit with filgastim\": \"Adverse_event\",\n",
    "        \"Since SS is a clinical diagnosis, heightened clinician awareness of the possibility of SS among patients receiving SSRI or mirtazapine in combination with opioids may lead to earlier detection and avoidance of potentially lethal consequences\": \"Adverse_event\",\n",
    "        \"The mechanism by which sunitinib induces gynaecomastia is thought to be associated with an unknown direct action on breast hormonal receptors\": \"Adverse_event\",\n",
    "        \"Intravitreal triamcinolone may have had an influence on the exacerbation of retinochoroiditis in the posterior pole of the patient\": \"Adverse_event\",\n",
    "        \"We evaluated a patient who developed a psychotic disorder after 4 months of isoniazid prophylaxis for a positive tuberculosis tine test\": \"Adverse_event\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814e13fa-1565-417e-afb9-eea025d97f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adverse_event' 'Potential_therapeutic_event']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [00:01, 32.96it/s]\n",
      "Some layers from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.3944 - accuracy: 0.8929"
     ]
    }
   ],
   "source": [
    "class BertClassifier(tf.keras.Model):\n",
    "    def __init__(self, num_classes,event_types, bert_model_name):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = TFAutoModel.from_pretrained(bert_model_name)\n",
    "        self.dense = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "        self.custom_att = tf.Variable(event_types,trainable=False)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        input_ids = inputs['input_ids']\n",
    "        attn_masks = inputs['attention_mask']\n",
    "        bert_embds = self.bert(input_ids, attention_mask=attn_masks)[1]\n",
    "        outputs = self.dense(bert_embds)\n",
    "        return outputs\n",
    "    \n",
    "# Create a pandas dataframe using the dictionary\n",
    "df = pd.DataFrame(training_data.items(), columns=['context', 'event_type'])\n",
    "\n",
    "# Get the unique event_type values from your dataframe\n",
    "event_types = df['event_type'].unique()\n",
    "# event_types_sorted = sorted(event_types)\n",
    "\n",
    "print(event_types)\n",
    "\n",
    "# Convert the event_type column to a categorical column\n",
    "df['event_type_categorical'] = pd.Categorical(df['event_type'], categories=event_types)\n",
    "# Convert the categorical column to one-hot encoding using tf.keras.utils.to_categorical()\n",
    "labels = tf.keras.utils.to_categorical(df['event_type_categorical'].cat.codes, num_classes=len(event_types))\n",
    "\n",
    "# Creating a tokenizer object from the BERT-base-cased model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "# Setting the maximum sequence length for the tokenized input\n",
    "maxLength = 128\n",
    "# Defining a function to generate the training data\n",
    "def generate_training_data(df, ids, masks, tokenizer):\n",
    "    for i, text in tqdm(enumerate(df['context'])):\n",
    "        # Tokenizing the input text using the BERT tokenizer\n",
    "        tokenized_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=maxLength,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        # Storing the tokenized input and attention masks in numpy arrays\n",
    "        ids[i, :] = tokenized_text.input_ids\n",
    "        masks[i, :] = tokenized_text.attention_mask\n",
    "    return ids, masks\n",
    "\n",
    "# Creating numpy arrays to store the tokenized input and attention masks\n",
    "X_input_ids = np.zeros((len(df), maxLength), dtype=np.int32)\n",
    "X_attn_masks = np.zeros((len(df), maxLength), dtype=np.int32)\n",
    "\n",
    "# Generating the training data using the generate_training_data function\n",
    "X_input_ids, X_attn_masks = generate_training_data(df, X_input_ids, X_attn_masks, tokenizer)\n",
    "\n",
    "# Creating a TensorFlow dataset object from the input and target data\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n",
    "\n",
    "# Defining a function to map the input and target data to the required format for the TensorFlow dataset\n",
    "def MapFunction(input_ids, attn_masks, labels):\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attn_masks\n",
    "    }, labels\n",
    "\n",
    "# Mapping the dataset to the required format using the SentimentDatasetMapFunction\n",
    "dataset = dataset.map(MapFunction)\n",
    "# Shuffling and batching the dataset for training the model\n",
    "dataset = dataset.shuffle(len(df)).batch(4, drop_remainder=False)\n",
    "# Splitting the dataset into training and validation sets\n",
    "p = 0.9\n",
    "train_size = int((len(df) // 4) * p)\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)\n",
    "# Create model instance and compile\n",
    "model = BertClassifier(num_classes=len(event_types),event_types = event_types, bert_model_name='emilyalsentzer/Bio_ClinicalBERT')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
    "accuracy_metric = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss_func, metrics=[accuracy_metric])\n",
    "# Train the model\n",
    "hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=1\n",
    ")\n",
    "# save the model using saved_model\n",
    "# tf.saved_model.save(model, 'PHEE_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72461ed3-5e65-4564-834f-14de0e89cb6b",
   "metadata": {},
   "source": [
    "<h3>Saving the Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f084079-7820-4bac-8498-2b09fc0c966c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: PHEE_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: PHEE_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# save the model using saved_model\n",
    "tf.saved_model.save(model, 'PHEE_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fda571-202b-4305-b0d5-75a948fa6506",
   "metadata": {},
   "source": [
    "<h3>Loading the Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0045559f-68bd-4288-bc25-8f74ce469af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Load the saved model\n",
    "model = tf.saved_model.load('PHEE_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e772cd-8c0b-4f84-9d3c-a09f57431fea",
   "metadata": {},
   "source": [
    "<h2>Prediction</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa956cba-670b-4893-8d6a-631fb78fc295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PrepareUser\\miniconda3\\envs\\prepareNLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "# Load the saved model\n",
    "event_model = tf.saved_model.load('PHEE_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "890220bf-383e-4278-bb08-5e1169f16893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(input_text, tokenizer):\n",
    "    token = tokenizer.encode_plus(\n",
    "        input_text,\n",
    "        max_length=maxLength, \n",
    "        truncation=True, \n",
    "        padding='max_length', \n",
    "        add_special_tokens=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    return {\n",
    "        'input_ids': tf.cast(token.input_ids, tf.int32),\n",
    "        'attention_mask': tf.cast(token.attention_mask, tf.int32)\n",
    "    }\n",
    "def make_prediction(model, processed_data, event_types):\n",
    "    probs = model({\n",
    "    'input_ids': processed_data['input_ids'],\n",
    "    'attention_mask': processed_data['attention_mask']})\n",
    "    #Note change the threshold based on perfromance of model recognizing the event\n",
    "    threshold = 0.55\n",
    "    if np.max(probs) < threshold:\n",
    "        prediction = 'Fallback Class'\n",
    "    else:\n",
    "        prediction = event_types[np.argmax(probs)]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39be9917-f833-4954-8f5e-f2aa2baded49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adverse_event', 'Potential_therapeutic_event']\n",
      "Adverse_event\n"
     ]
    }
   ],
   "source": [
    "event_types_encoded = event_model.custom_att.numpy()\n",
    "event_types = [event.decode('utf-8') for event in event_types_encoded]\n",
    "print(event_types)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "maxLength = 128\n",
    "input_text = \"A 60-year-old white man with chronic bronchitis was noted to develop acute respiratory failure and metabolic acidosis four days after being started on methazolamide (Neptazane) for an ophthalmologic problem\"\n",
    "processed_data = prepare_data(input_text, tokenizer)\n",
    "nlp_predicted_event = make_prediction(event_model, processed_data=processed_data, event_types=event_types)\n",
    "print(nlp_predicted_event)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cbefb8-98a9-41e2-93c7-ae59a61f17c0",
   "metadata": {},
   "source": [
    "<h2>Prediction on Prepare Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1d38248-8b96-4ca7-8f98-092f5213a085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PrepareUser\\miniconda3\\envs\\prepareNLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries and load odel\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "maxLength = 128\n",
    "# Load the saved model\n",
    "event_model = tf.saved_model.load('Models/131/131scenario1_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "220ef03a-3dbd-472a-8d3f-09a434bb3408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(input_text, tokenizer):\n",
    "    token = tokenizer.encode_plus(\n",
    "        input_text,\n",
    "        max_length=maxLength, \n",
    "        truncation=True, \n",
    "        padding='max_length', \n",
    "        add_special_tokens=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    return {\n",
    "        'input_ids': tf.cast(token.input_ids, tf.int32),\n",
    "        'attention_mask': tf.cast(token.attention_mask, tf.int32)\n",
    "    }\n",
    "def make_prediction(model, processed_data, event_types):\n",
    "    probs = model({\n",
    "    'input_ids': processed_data['input_ids'],\n",
    "    'attention_mask': processed_data['attention_mask']})\n",
    "    #Note change the threshold based on perfromance of model recognizing the event\n",
    "    threshold = 0.55\n",
    "    # if np.max(probs) < threshold:\n",
    "    #     prediction = 'Fallback Class'\n",
    "    # else:\n",
    "    #     prediction = event_types[np.argmax(probs)]\n",
    "    # return prediction\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c85ded51-2f3b-4579-b528-fed7c1cc697b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepsis', 'medical history', 'cardiac activity']\n",
      "Fallback Class\n"
     ]
    }
   ],
   "source": [
    "event_types_encoded = event_model.custom_att.numpy()\n",
    "event_types = [event.decode('utf-8') for event in event_types_encoded]\n",
    "print(event_types)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "maxLength = 128\n",
    "input_text = \"sepsis is a medical condition\"\n",
    "processed_data = prepare_data(input_text, tokenizer)\n",
    "nlp_predicted_event = make_prediction(event_model, processed_data=processed_data, event_types=event_types)\n",
    "print(nlp_predicted_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007a7a1-a970-4810-95e1-4623fe298201",
   "metadata": {},
   "source": [
    "<h2>Prediction on Sentiment Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d09b4346-aea6-4da2-b84f-77a66f4c3340",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries and load odel\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "maxLength = 128\n",
    "# Load the saved model\n",
    "sentiment_model = tf.saved_model.load('sentimentModelPrepare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b583f9f8-ccf5-4240-bad2-9c20208087d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(input_text, tokenizer):\n",
    "    token = tokenizer.encode_plus(\n",
    "        input_text,\n",
    "        max_length=maxLength, \n",
    "        truncation=True, \n",
    "        padding='max_length', \n",
    "        add_special_tokens=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    return {\n",
    "        'input_ids': tf.cast(token.input_ids, tf.int32),\n",
    "        'attention_mask': tf.cast(token.attention_mask, tf.int32)\n",
    "    }\n",
    "def make_sentiment_prediction(model, processed_data, sentiment_labels):\n",
    "    probs = model({\n",
    "    'input_ids': processed_data['input_ids'],\n",
    "    'attention_mask': processed_data['attention_mask']})\n",
    "    #Note change the threshold based on perfromance of model recognizing the event\n",
    "    threshold = 0.55\n",
    "    if np.max(probs) < threshold:\n",
    "        prediction = 'Fallback Class'\n",
    "    else:\n",
    "        prediction = sentiment_labels[np.argmax(probs)]\n",
    "    return prediction\n",
    "    # return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed744caa-3b83-406c-bf2c-0a254dd482b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sadness', 'anger', 'love', 'surprise', 'fear', 'joy']\n",
      "anger\n"
     ]
    }
   ],
   "source": [
    "sentiment_labels_encoded = sentiment_model.custom_att.numpy()\n",
    "sentiment_labels = [label.decode('utf-8') for label in sentiment_labels_encoded]\n",
    "print(sentiment_labels)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "maxLength = 128\n",
    "input_text = \"I am really angry with you\"\n",
    "processed_data = prepare_data(input_text, tokenizer)\n",
    "predicted_sentiment =make_sentiment_prediction(sentiment_model, processed_data=processed_data, sentiment_labels = sentiment_labels)\n",
    "print(predicted_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb75131-1977-4480-b1d4-8a6a74665d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22568174-d660-4529-bbad-1765fafbc6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b79196-0e0a-4a96-bcb2-c8d8edf58742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
